{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vex99np2wFVt"
   },
   "source": [
    "# 03. PyTorch Computer Vision Exercises\n",
    "\n",
    "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
    "\n",
    "They're a bunch of fun.\n",
    "\n",
    "You're going to get to write plenty of code!\n",
    "\n",
    "## Resources\n",
    "\n",
    "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
    "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
    "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
    "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaeYzOTLwWh2",
    "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 26 12:09:06 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P3             26W /   55W |       0MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DNwZLMbCzJLk",
    "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agnie\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Exercises require PyTorch > 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# TODO: Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSFX7tc1w-en"
   },
   "source": [
    "## 1. What are 3 areas in industry where computer vision is currently being used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyWRkvWGbCXj"
   },
   "source": [
    "Currently, computer vision is used in many areas in industry, eg: \n",
    "- medicine: navigation of medical tools;\n",
    "- automotive: support for drivers: recognition of others cars, pedestrians etc;\n",
    "- fitness and sports: self-tracking system helping to exercise correctly\n",
    "- education, ecommerce, agriculture etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBK-WI6YxDYa"
   },
   "source": [
    "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1rxD6GObCqh"
   },
   "source": [
    "Overfitting is a phenomenon in machine learning, when the model is too fitted to training data, and it loses the ability to generalize (= achieving satisfying results on testing data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeYFEqw8xK26"
   },
   "source": [
    "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
    "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocvOdWKcbEKr"
   },
   "source": [
    "There are many possible ways of preventing overfitting in machine learning. The most common are:\n",
    "- use more training data - when a model is trained on a larger dataset, it can learn a broader range of patterns, making it less likely to memorize the noise or specific quirks of a smaller dataset\n",
    "- use regularizations - regularization techniques add a penalty to the loss function used to train the model, discouraging overly complex models that may overfit the training data. The most common regularizations are: L1, L2 and Dropout layern. \n",
    "- use less complex model (= simplify it) - simplifying the model involves reducing the number of parameters or features. This can be achieved by: reducing the number of features (like hidden neurons) or chosing a simpler model architecture. Simpler models are less likely to capture noise and overfit, as they have a lower capacity to model complex patterns in the data.\n",
    "- use data augmentation - data augmentation involves artificially increasing the size of the training dataset by creating modified versions of existing data points. This is especially common in image processing, where techniques such as rotation, scaling, flipping, and cropping are used to create new training samples. \n",
    "- use early stopping - it pauses the training phase before the machine learning model learns the noise in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKdEEFEqxM-8"
   },
   "source": [
    "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqZaJIRMbFtS"
   },
   "source": [
    "Here are a few reasons why the later layers in a CNN see more of the image than the earlier layers:\n",
    "- Hierarchical feature learning: CNNs are designed to learn hierarchical representations of features in an image. The early layers of a CNN learn low-level features such as edges and textures, while the deeper layers learn high-level features such as shapes and objects. By gradually increasing the receptive field of the neurons in the network, the later layers can capture more global information and context about the image, which is essential for making accurate predictions.\n",
    "- Spatial hierarchy: In an image, local features (such as edges and corners) are combined to form more complex patterns and objects at higher levels of abstraction. By increasing the receptive field of the neurons in the later layers, the network can capture these spatial hierarchies and relationships between different parts of the image, leading to better representation learning.\n",
    "- Downsampling: Pooling layers are often used in CNNs to downsample the feature maps and reduce the spatial dimensions while retaining important features. As the network progresses through the layers, the spatial dimensions of the feature maps decrease due to the pooling operations, allowing the later layers to have a broader view of the input image.\n",
    "- Translation invariance: By allowing the later layers to have a larger receptive field, the network can achieve translation invariance, which means that the network's predictions are not sensitive to small translations in the input image. This property is crucial for tasks such as object recognition, where the position of an object in the image should not affect the network's ability to recognize it.\n",
    "\n",
    "In summary, the design of CNNs with increasing receptive fields in the later layers helps the network learn hierarchical representations of features, capture spatial hierarchies in the input image, downsample the feature maps for efficiency, and achieve translation invariance, all of which contribute to the network's ability to effectively learn and recognize patterns in images.\n",
    "\n",
    "Source: https://www.quora.com/In-a-CNN-why-do-the-later-layers-see-more-of-the-image-than-the-earlier-layers-Shouldnt-it-be-the-other-way-around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvf-3pODxXYI"
   },
   "source": [
    "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SHjeuN81bHza"
   },
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.MNIST(\n",
    "                            root=\"data\", \n",
    "                            train=True, \n",
    "                            download=True, \n",
    "                            transform=ToTensor(), \n",
    "                            target_transform=None \n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.MNIST(\n",
    "                            root=\"data\",\n",
    "                            train=False, \n",
    "                            download=True,\n",
    "                            transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 60000 10000 10000\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets))\n",
    "\n",
    "class_names = train_data.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxZW-uAbxe_F"
   },
   "source": [
    "## 6. Visualize at least 5 different samples of the MNIST training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QVFsYi1PbItE"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 1, 5\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPDzW0wxhi3"
   },
   "source": [
    "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ALA6MPcFbJXQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x00000225273F78F0>, <torch.utils.data.dataloader.DataLoader object at 0x000002252733C890>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True \n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False \n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCCVfXk5xjYS"
   },
   "source": [
    "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyVGG(\n",
      "  (block_1): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "TinyVGG(\n",
      "  (block_1): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "                                    nn.Conv2d(in_channels=input_shape, \n",
    "                                              out_channels=hidden_units, \n",
    "                                              kernel_size=3, \n",
    "                                              stride=1, \n",
    "                                              padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=hidden_units, \n",
    "                                              out_channels=hidden_units,\n",
    "                                              kernel_size=3,\n",
    "                                              stride=1,\n",
    "                                              padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2,\n",
    "                                                 stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "                                    nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(in_features=hidden_units*7*7, \n",
    "                                                  out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model0_gpu = TinyVGG(input_shape=1, \n",
    "                      hidden_units=10, \n",
    "                      output_shape=len(class_names)).to(device)\n",
    "print(model0_gpu)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model0_cpu = TinyVGG(input_shape=1, \n",
    "                      hidden_units=10, \n",
    "                      output_shape=len(class_names))\n",
    "print(model0_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_3zUr7xlhy"
   },
   "source": [
    "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jSo6vVWFbNLD"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        # train_acc += accuracy_fn(y_true=y,\n",
    "        #                          y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "        train_acc += accuracy_fn(y_pred.argmax(dim=1), \n",
    "                                 y) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            # test_acc += accuracy_fn(y_true=y,\n",
    "            #                         y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            test_acc += accuracy_fn(test_pred.argmax(dim=1),\n",
    "                                    y)\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.30087 | Train accuracy: 0.90%\n",
      "Test loss: 0.06981 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [07:46<31:04, 466.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.07938 | Train accuracy: 0.98%\n",
      "Test loss: 0.05678 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [15:49<23:48, 476.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.06120 | Train accuracy: 0.98%\n",
      "Test loss: 0.05372 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [24:41<16:43, 501.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.05311 | Train accuracy: 0.98%\n",
      "Test loss: 0.04937 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [33:23<08:29, 509.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.04640 | Train accuracy: 0.99%\n",
      "Test loss: 0.04384 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [41:13<00:00, 494.73s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_time_end_on_cpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     22\u001b[0m     test_step(data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m     23\u001b[0m                             model\u001b[38;5;241m=\u001b[39mmodel0_cpu,\n\u001b[0;32m     24\u001b[0m                             loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     25\u001b[0m                             accuracy_fn\u001b[38;5;241m=\u001b[39maccuracy_fn, \n\u001b[0;32m     26\u001b[0m                             device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m     28\u001b[0m train_time_end_on_gpu \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m     29\u001b[0m total_train_time_model_1 \u001b[38;5;241m=\u001b[39m print_train_time(start\u001b[38;5;241m=\u001b[39mtrain_time_start_on_cpu,\n\u001b[1;32m---> 30\u001b[0m                                             end\u001b[38;5;241m=\u001b[39mtrain_time_end_on_cpu,\n\u001b[0;32m     31\u001b[0m                                             device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_time_end_on_cpu' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "accuracy_fn = Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(params=model0_cpu.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "                            model=model0_cpu, \n",
    "                            loss_fn=loss_fn,\n",
    "                            optimizer=optimizer,\n",
    "                            accuracy_fn=accuracy_fn, \n",
    "                            device = device)\n",
    "    test_step(data_loader=test_dataloader,\n",
    "                            model=model0_cpu,\n",
    "                            loss_fn=loss_fn,\n",
    "                            accuracy_fn=accuracy_fn, \n",
    "                            device = device)\n",
    "\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time on cpu: 2473.661 seconds\n"
     ]
    }
   ],
   "source": [
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.28221 | Train accuracy: 0.91%\n",
      "Test loss: 0.06469 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:15<01:00, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.07446 | Train accuracy: 0.98%\n",
      "Test loss: 0.04805 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:49<01:18, 26.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.05457 | Train accuracy: 0.98%\n",
      "Test loss: 0.05122 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:25<01:01, 30.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "---------\n",
      "Train loss: 0.04629 | Train accuracy: 0.98%\n",
      "Test loss: 0.04949 | Test accuracy: 0.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:00<00:32, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "---------\n",
      "Train loss: 0.03960 | Train accuracy: 0.99%\n",
      "Test loss: 0.03645 | Test accuracy: 0.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:34<00:00, 30.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time on cuda: 156.026 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "accuracy_fn = Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(params=model0_gpu.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "                            model=model0_gpu, \n",
    "                            loss_fn=loss_fn,\n",
    "                            optimizer=optimizer,\n",
    "                            accuracy_fn=accuracy_fn,\n",
    "                            device = device)\n",
    "    test_step(data_loader=test_dataloader,\n",
    "                            model=model0_gpu,\n",
    "                            loss_fn=loss_fn,\n",
    "                            accuracy_fn=accuracy_fn,\n",
    "                            device = device)\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using GPU causes the dramatically degrease of training time:\n",
    "- cpu: 2473 seconds\n",
    "- gpu: 156 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1CsHhPpxp1w"
   },
   "source": [
    "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_plot = 5 \n",
    "for i in range(num_to_plot):\n",
    "  # Get image and labels from the test data\n",
    "  img = test_data[i][0]\n",
    "  label = test_data[i][1]\n",
    "\n",
    "  # Make prediction on image\n",
    "  model_pred_logits = model_gpu(img.unsqueeze(dim=0).to(device))\n",
    "  model_pred_probs = torch.softmax(model_pred_logits, dim=1)\n",
    "  model_pred_label = torch.argmax(model_pred_probs, dim=1)\n",
    "\n",
    "  # Plot the image and prediction\n",
    "  plt.figure()\n",
    "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "  plt.title(f\"Truth: {label} | Pred: {model_pred_label.cpu().item()}\") \n",
    "  plt.axis(False);\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 1, 5\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQwzqlBWxrpG"
   },
   "source": [
    "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSrXiT_AbQ6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj6bDhoWxt2y"
   },
   "source": [
    "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leCTsqtSbR5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHS20cNTxwSi"
   },
   "source": [
    "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
    "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
    "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
    "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78a8LjtdbSZj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_pytorch_computer_vision_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
